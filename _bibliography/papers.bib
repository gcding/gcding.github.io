---
---
@ARTICLE{Ding_2022_TGRS,
    abbr      = {TGRS},
    bibtex_show={true},
    html      = {https://doi.org/10.1109/TGRS.2022.3208326},
    pdf       = {paper/TGRS2022.pdf},
    selected  = {false},
    abstract  = {Object counting has attracted a lot of attention in remote sensing image analysis. In density map based object counting algorithms, the ground truth density maps generated by fix-sized Gaussian kernels ignore the spatial features of the objects. In this paper, an Adaptive Density Map Assisted Learning algorithm (ADMAL) is proposed, which taps into spatial features of the objects from the beginning phase of ground truth density map generation. ADMAL consists of two networks: a Contexture Aware Density Map Generation (CADMG) network and a Transformer-based Density Map Estimation (TDME) network. The CADMG network is designed to generate a ground truth density map from each annotated point map. Comparing with Gaussian convolved density maps, the ground truth density maps generated by CADMG will be tailored according to the texture and neighborhood relationship among objects, which can promote the learning effect of the TDME network. TDME is the core network for object counting. The backbone of the TDME network adopts a Swin transformer structure, the self-attention mechanism of which possesses a larger receptive field for effective feature extraction in remote sensing images. Comprehensive experiments prove that the ground truth density map generated by CADMG can help various density map estimation networks achieve better training effects, among which TDME achieves the best performances. Moreover, the ADMAL algorithm can achieve preferable object counting performances for both satellite-based image and drone-based image. Code and pre-trained models are available at https://github.com/gcding/ADMAL-pytorch.},
    author    = {Guanchen Ding and Mingpeng Cui and Daiqin Yang and Tao Wang and Sihan Wang and Yunfei Zhang},
    journal   = {IEEE Transactions on Multimedia}, 
    title     = {Crowd counting via unsupervised cross-domain feature adaptation}, 
    year      = {2022},
    volume    = {60},
    number    = {},
    pages     = {1--11},
    doi       = {10.1109/TGRS.2022.3208326}
}

@ARTICLE{Ding_2022_TMM,
    abbr      = {TMM},
    bibtex_show={true},
    html      = {https://ieeexplore.ieee.org/abstract/document/9788041},
    pdf       = {paper/TMM2022.pdf},
    selected  = {false},
    abstract  = {Given an image, crowd counting aims to estimate the amount of target objects in the image. With un-predictable installation situations of surveillance systems (or other equipment), crowd counting images from different data sets may exhibit severe discrepancies in viewing angle, scale, lighting condition, etc. As it is usually expensive and time-consuming to annotate each data set for model training, it has been an essential issue in crowd counting to transfer a well-trained model on a labeled data set (source domain) to a new data set (target domain). To tackle this problem, we propose a cross-domain learning network to learn the domain gaps in an unsupervised learning manner. The proposed network comprises of a Multi-granularity Feature-aware Discriminator (MFD) module, a Domain-Invariant Feature Adaptation (DFA) module, and a Cross-domain Vanishing Bridge (CVB) module to remove domain-specific information from the extracted features and promote the mapping performances of the network. Unlike most existing methods that use only Global Feature Discriminator (GFD) to align features at image level, an additional Local Feature Discriminator (LFD) is inserted and together with GFD form the MFD module. As a complement to MFD, LFD refines features at pixel level and has the ability to align local features. The DFA module explicitly measures the distances between the source domain features and the target domain features and aligns the marginal distribution of their features with Maximum Mean Discrepancy (MMD). Finally, the CVB module provides an incremental capability of removing the impact of interfering part of the extracted features. Several well-known networks are adopted as the backbone of our algorithm to prove the effectiveness of the proposed adaptation structure. Comprehensive experiments demonstrate that our model achieves competitive performance to the state-of-the-art methods. Code and pre-trained models are available at https://github.com/gcding/CDFA-pytorch.},
    author    = {Ding, Guanchen and Yang, Daiqin and Wang, Tao and Wang, Sihan and Zhang, Yunfei},
    journal   = {IEEE Transactions on Multimedia}, 
    title     = {Crowd counting via unsupervised cross-domain feature adaptation}, 
    year      = {2022},
    volume    = {},
    number    = {},
    pages     = {1-1},
    doi       = {10.1109/TMM.2022.3180222}
}

@InProceedings{Ding_2022_CVPR,
    abbr      = {CVPRW},
    bibtex_show={true},
    html      = {https://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Ding_A_Coarse-To-Fine_Boundary_Localization_Method_for_Naturalistic_Driving_Action_Recognition_CVPRW_2022_paper.html},
    pdf       = {paper/CVPRW2022_AICity.pdf},
    selected  = {false},
    abstract  = {Naturalistic driving action recognition plays an important role in understanding drivers' distraction behavior in the traffic environment. The main challenge of this task is the accurate localization of the temporal boundary for each distraction driving behavior in the video. Although many temporal action localization methods can identify action classes, it is difficult to predict accurate temporal boundaries for this task since the driving actions of the same category usually present large intra-class variation. In this paper, we introduce a Coarse-to-Fine Boundary Localization method called CFBL, which obtains fine-grained temporal boundaries progressively through three stages. Concretely, in the first coarse boundary generation stage, we adopt a modified anchor-free model Anchor-Free Saliency-based Detector (AFSD) to make an interval estimation of the temporal boundaries of distraction behavior. In the second boundary refinement stage, we use the Dense Boundary Generation (DBG) model to adjust the estimated interval of the temporal boundaries. In the final boundary decision stage, we build a Localization Boundary Refinement Module to determine the final boundaries of different actions. Besides, we adopt a voting strategy to combine the results of different camera views to enhance the model's distraction driving action classification ability. The experiments conducted on the Track 3 validation set of the 2022 AI City Challenge demonstrate competitive performance of the proposed method.},
    author    = {Ding, Guanchen* and Han, Wenwei* and Wang, Chenglong* and Cui, Mingpeng and Zhou, Lin and Pan, Dianbo and Wang, Jiayi and Zhang, Junxi and Zhenzhong, Chen},
    title     = {A Coarse-To-Fine Boundary Localization Method for Naturalistic Driving Action Recognition},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {June},
    year      = {2022},
    pages     = {3234-3241}
}

@InProceedings{Liu_2021_ICCV,
    abbr      = {ICCVW},
    bibtex_show={true},
    html      = {https://openaccess.thecvf.com/content/ICCV2021W/VisDrone/html/Liu_VisDrone-CC2021_The_Vision_Meets_Drone_Crowd_Counting_Challenge_Results_ICCVW_2021_paper.html},
    pdf       = {paper/ICCVW2021_VISDrone.pdf},
    selected  = {false},
    abstract  = {Crowding counting research evolves quickly by the leverage of development in deep learning. Many researchers put their efforts into crowd counting tasks and have achieved many significant improvements. However, current datasets still barely satisfy this evolution and high quality evaluation data is urgent. Motivated by high quality and quantity study in crowding counting, we collect a drone-captured dataset formed by 5,468 images(images in RGB and thermal appear in pairs and 2,734 respectively). There are 1,807 pairs of images for training, and 927 pairs for testing. We manually annotate persons with points in each frame. Based on this dataset, we organized the Vision Meets Drone Crowd Counting Challenge(Visdrone-CC2021) in conjunction with the International Conference on Computer Vision (ICCV 2021). Our challenge attracts many researchers to join, which pave the road of speed up the milestone in crowding counting. To summarize the competition, we select the most remarkable algorithms from participants' submissions and provide a detailed analysis of the evaluation results. More information can be found at the website: http://www.aiskyeye.com/.},
    author    = {Liu, Zhihao and He, Zhijian and Wang, Lujia and Wang, Wenguan and Yuan, Yixuan and Zhang, Dingwen and Zhang, Jinglin and Zhu, Pengfei and Van Gool, Luc and Han, Junwei and Hoi, Steven and Hu, Qinghua and Liu, Ming and Pan, Junwen},
    title     = {VisDrone-CC2021: The Vision Meets Drone Crowd Counting Challenge Results},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
    month     = {October},
    year      = {2021},
    pages     = {2830-2838}
}

@InProceedings{Chen_2021_CVPR,
    abbr      = {CVPRW},
    bibtex_show={true},
    html      = {https://openaccess.thecvf.com/content/CVPR2021W/AICity/html/Chen_Dual-Modality_Vehicle_Anomaly_Detection_via_Bilateral_Trajectory_Tracing_CVPRW_2021_paper.html},
    pdf       = {paper/CVPRW2021_AICity.pdf},
    selected  = {false},
    abstract  = {Traffic anomaly detection has played a crucial role in Intelligent Transportation System (ITS). The main challenges of this task lie in the highly diversified anomaly scenes and variational lighting conditions. Although much work has managed to identify the anomaly in homogenous weather and scene, few resolved to cope with complex ones. In this paper, we proposed a dual-modality modularized methodology for the robust detection of abnormal vehicles. We introduced an integrated anomaly detection framework comprising the following modules: background modeling, vehicle tracking with detection, mask construction, Region of Interest (ROI) backtracking, and dual-modality tracing. Concretely, we employed background modeling to filter the motion information and left the static information for later vehicle detection. For the vehicle detection and tracking module, we adopted YOLOv5 and multi-scale tracking to localize the anomalies. Besides, we utilized the frame difference and tracking results to identify the road and obtain the mask. In addition, we introduced multiple similarity estimation metrics to refine the anomaly period via backtracking. Finally, we proposed a dual-modality bilateral tracing module to refine the time further. The experiments conducted on the Track 4 testset of the NVIDIA 2021 AI City Challenge yielded a result of 0.9302 F1-Score and 3.4039 root mean square error (RMSE), indicating the effectiveness of our framework.},
    author    = {Chen, Jingyuan* and Ding, Guanchen* and Yang, Yuchen* and Han, Wenwei and Xu, Kangmin and Gao, Tianyi and Zhang, Zhe and Ouyang, Wanping and Cai, Hao and Zhenzhong, Chen},
    title     = {Dual-Modality Vehicle Anomaly Detection via Bilateral Trajectory Tracing},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {June},
    year      = {2021},
    pages     = {4016-4025}
}

@INPROCEEDINGS{Huang_2020_VCIP,
    abbr      = {VCIP},
    bibtex_show={true},
    html      = {https://ieeexplore.ieee.org/abstract/document/9301785},
    pdf       = {},
    selected  = {false},
    abstract  = {Car counting on drone-based images is a challenging task in computer vision. Most advanced methods for counting are based on density maps. Usually, density maps are first generated by convolving ground truth point maps with a Gaussian kernel for later model learning (generation). Then, the counting network learns to predict density maps from input images (estimation). Most studies focus on the estimation problem while overlooking the generation problem. In this paper, a training framework is proposed to generate density maps by learning and train generation and estimation subnetworks jointly. Experiments demonstrate that our method outperforms other density map-based methods and shows the best performance on drone-based car counting.},
    author    = {Huang, Jingxian* and Ding, Guanchen* and Guo, Yujia and Yang, Daiqin and Wang, Sihan and Wang, Tao and Zhang, Yunfei},
    booktitle = {2020 IEEE International Conference on Visual Communications and Image Processing (VCIP)}, 
    title     = {Drone-Based Car Counting via Density Map Learning}, 
    year      = {2020},
    volume    = {},
    number    = {},
    pages     = {239-242},
    doi       = {10.1109/VCIP49819.2020.9301785}
}